---
title: Project 2 - Exoplanet Habitability
date: 2024-02-19
categories: 
tags: # TAG names should always be lowercase
math: true
image:
  path: /assets/proxima.jpg
---

## Introduction to the Data
I found a data set that consists of roughly 4,000 planets outside of our Solar System. The dataset has 118 columns which cover statistics for both a planet and its nearest star. Some of these include but are not limited to: gravity, distance from star, temperature, star mass, planet type and tidal lock. Most importantly, there is a column making an estimate on the habitability of the planet. With this dataset, I would like to make a classification model to predict the habitability of any given exoplanet. I originally found the data on Kaggle, but found an updated version of the dataset from the orignal source (the University of Puerto Rico), which compiled the data from the NASA exoplanet archives.

## Pre-processing
Overall, this data set was fairly clean. Out of 118 columns, there were a few that were completly null, which I removed. Other than this, I removed a few uncessary columns that consisted of planet, constellation and star names. 

Next, I removed all columns that had 'MIN' and 'MAX' in them. This is because there were a few columns relating to a the potential error in NASA's math caculations. For example, the column P_MASS_ERROR_MAX (max error for the estimated planet mass). These will have no effect on planet habitability. Interestingly, before these had been pruned, my models incorrectly decided that these as columns had high relation to habitability. 

Next, I used dummies to set columns with the type 'object' to boolean values. 

Lastly, set x to be habitability, y to every other column, then split the data into training and testing sets. I chose a split of 80% training to 20% testing. 
## Data Visualization

## Modeling
I chose to model the data using a decision tree. I chose this model because decision trees are extremely easy to understand, even for those who do not have a background in stats or CS. In this tree, the x variable (independent) is habitability and y is all the other columns. 
![Tree3](assets/df4.png)
Orange is uninhabitable, purple is highly likely to be habitable, green is possibly habitable.

## Evaluation
Overall, my model was strong. Each time it ran, I had an accuracy score of at least 98%, usually closer to the end of 99% accuracy.
![Accuracy](assets/accuracy.png)

This is the accuracy of my first decision tree. A score .99654 is very good.
## Storytelling

## Impact
There are a few impacts that could result from this data. The main positive impact, is that the data could be used to predict which planets are likely habitable. When new planets are discovered, the habitability of newly discovered planets can be predicted. If planets  have incorrect habitability estimations, a few negative effects may arise. First, resources dedicated to space exploration may be sent to planets that turn out to be uninhabitable. Second, highly habitable planets could be ignored if they do not fit our current knowledge of habitability. We only have one planet we can truly base our habitability measurements off of, Earth. Furthermore, we only have species from our planet to base our understanding of life on. There may exist conditions or factors that support life that we currently do not understand yet.  

[Data](https://phl.upr.edu/hwc/data)
## Code

